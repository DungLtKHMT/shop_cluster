# üîÑ H∆Ø·ªöNG D·∫™N LU·ªíNG X·ª¨ L√ù V√Ä ƒêI·ªÄU CH·ªàNH THAM S·ªê

## üìä T·ªîNG QUAN PIPELINE

D·ª± √°n ph√¢n c·ª•m kh√°ch h√†ng d·ª±a tr√™n lu·∫≠t k·∫øt h·ª£p bao g·ªìm 6 b∆∞·ªõc ch√≠nh:

```
[1] Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
    ‚Üì
[2] Chu·∫©n b·ªã Basket (gi·ªè h√†ng)
    ‚Üì
[3] Khai ph√° lu·∫≠t k·∫øt h·ª£p (Apriori/FP-Growth)
    ‚Üì
[4] Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ lu·∫≠t
    ‚Üì
[5] Ph√¢n c·ª•m kh√°ch h√†ng (K-Means)
    ‚Üì
[6] Ph√¢n t√≠ch v√† Di·ªÖn gi·∫£i k·∫øt qu·∫£
```

---

## üîç CHI TI·∫æT T·ª™NG B∆Ø·ªöC

### **B∆Ø·ªöC 1: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu**
üìÅ **Notebook**: `preprocessing_and_eda.ipynb`  
üîß **Class**: `DataCleaner`

#### **Ch·ª©c nƒÉng:**
- Load d·ªØ li·ªáu t·ª´ file CSV g·ªëc
- L√†m s·∫°ch d·ªØ li·ªáu:
  - Lo·∫°i b·ªè h√≥a ƒë∆°n h·ªßy (InvoiceNo b·∫Øt ƒë·∫ßu b·∫±ng 'C')
  - Ch·ªâ gi·ªØ kh√°ch h√†ng UK
  - Lo·∫°i b·ªè Quantity ‚â§ 0 ho·∫∑c UnitPrice ‚â§ 0
  - B·ªè Description b·ªã thi·∫øu
- T·∫°o c·ªôt `TotalPrice = Quantity √ó UnitPrice`
- T√≠nh RFM (Recency, Frequency, Monetary)

#### **Output:**
- `data/processed/cleaned_uk_data.csv`

#### **Tham s·ªë ƒëi·ªÅu ch·ªânh:**

| Tham s·ªë | V·ªã tr√≠ | M·ª•c ƒë√≠ch | G·ª£i √Ω |
|---------|--------|----------|-------|
| `Country` filter | `clean_data()` | Ch·ªçn th·ªã tr∆∞·ªùng | C√≥ th·ªÉ th·ª≠ "Germany", "France" ho·∫∑c t·∫•t c·∫£ |
| `snapshot_date` | `compute_rfm()` | ƒêi·ªÉm m·ªëc t√≠nh Recency | M·∫∑c ƒë·ªãnh: max(InvoiceDate) + 1 ng√†y |
| Ng∆∞·ª°ng l·ªçc Quantity | `clean_data()` | Lo·∫°i giao d·ªãch b·∫•t th∆∞·ªùng | Hi·ªán t·∫°i: > 0, c√≥ th·ªÉ tƒÉng l√™n ‚â• 2 |

#### **üí° G·ª£i √Ω c·∫£i thi·ªán:**
- Th·ª≠ l·ªçc theo `Quantity < 100` ƒë·ªÉ lo·∫°i c√°c ƒë∆°n h√†ng b√°n bu√¥n qu√° l·ªõn
- Th·ª≠ l·ªçc theo `UnitPrice < 1000` ƒë·ªÉ lo·∫°i outliers v·ªÅ gi√°
- Ph√¢n t√≠ch theo m√πa (th√™m feature th√°ng, qu√Ω)

---

### **B∆Ø·ªöC 2: Chu·∫©n b·ªã Basket**
üìÅ **Notebook**: `basket_preparation.ipynb`  
üîß **Class**: `BasketPreparer`

#### **Ch·ª©c nƒÉng:**
- Chuy·ªÉn d·ªØ li·ªáu giao d·ªãch th√†nh ma tr·∫≠n boolean Invoice √ó Item
- M·ªói d√≤ng = 1 gi·ªè h√†ng (InvoiceNo)
- M·ªói c·ªôt = 1 s·∫£n ph·∫©m (Description)
- Gi√° tr·ªã: True n·∫øu s·∫£n ph·∫©m c√≥ trong gi·ªè, False n·∫øu kh√¥ng

#### **Output:**
- `data/processed/basket_bool.parquet`

#### **Tham s·ªë ƒëi·ªÅu ch·ªânh:**

| Tham s·ªë | V·ªã tr√≠ | M·ª•c ƒë√≠ch | G·ª£i √Ω |
|---------|--------|----------|-------|
| `invoice_col` | `__init__()` | ƒê·ªãnh nghƒ©a "basket" | C√≥ th·ªÉ d√πng CustomerID thay v√¨ InvoiceNo |
| `min_items` | `create_basket_matrix()` | L·ªçc basket qu√° nh·ªè | M·∫∑c ƒë·ªãnh: 1, ƒë·ªÅ xu·∫•t: 2-3 |
| `max_items` | `create_basket_matrix()` | L·ªçc basket qu√° l·ªõn | Kh√¥ng c√≥, n√™n th√™m ~50 |
| `min_support_item` | T√πy ch·ªânh | L·ªçc item xu·∫•t hi·ªán √≠t | Ch∆∞a c√≥, n√™n th√™m |

#### **üí° G·ª£i √Ω c·∫£i thi·ªán:**
- **L·ªçc basket size**: Ch·ªâ gi·ªØ gi·ªè c√≥ 2-50 items ƒë·ªÉ tr√°nh nhi·ªÖu
- **L·ªçc rare items**: Lo·∫°i items xu·∫•t hi·ªán < 0.1% baskets
- **Group items**: Gom nh√≥m s·∫£n ph·∫©m t∆∞∆°ng t·ª± (v√≠ d·ª•: "RED MUG", "BLUE MUG" ‚Üí "MUG")

---

### **B∆Ø·ªöC 3: Khai ph√° lu·∫≠t k·∫øt h·ª£p**
üìÅ **Notebook**: `apriori_modelling.ipynb`, `fp_growth_modelling.ipynb`  
üîß **Class**: `AssociationRulesMiner`, `FPGrowthMiner`

> ‚ö° **L∆ØU √ù QUAN TR·ªåNG**: Theo y√™u c·∫ßu ƒë·ªÅ b√†i, b·∫°n **CH·ªà C·∫¶N CH·ªåN 1 TRONG 2** thu·∫≠t to√°n (Apriori **HO·∫∂C** FP-Growth).  
> 
> **Khuy·∫øn ngh·ªã: D√πng FP-Growth** v√¨:
> - ‚úÖ **Nhanh h∆°n** Apriori (ƒë·∫∑c bi·ªát v·ªõi min_support th·∫•p)
> - ‚úÖ Kh√¥ng sinh candidate items ‚Üí ti·∫øt ki·ªám b·ªô nh·ªõ
> - ‚úÖ K·∫øt qu·∫£ t∆∞∆°ng ƒë∆∞∆°ng v·ªÅ ch·∫•t l∆∞·ª£ng lu·∫≠t
> - ‚úÖ File output: `rules_fpgrowth_filtered.csv` (ƒë√£ c√≥ s·∫µn trong d·ª± √°n)
>
> N·∫øu mu·ªën so s√°nh 2 thu·∫≠t to√°n ‚Üí L√†m ph·∫ßn **n√¢ng cao** (kh√¥ng b·∫Øt bu·ªôc)

#### **Ch·ª©c nƒÉng:**
- T√¨m t·∫≠p ph·ªï bi·∫øn (frequent itemsets)
- Sinh lu·∫≠t k·∫øt h·ª£p: Antecedent ‚Üí Consequent
- T√≠nh support, confidence, lift

#### **Output:**
- `data/processed/rules_apriori_filtered.csv` (n·∫øu d√πng Apriori)
- `data/processed/rules_fpgrowth_filtered.csv` ‚≠ê (n·∫øu d√πng FP-Growth - Khuy·∫øn ngh·ªã)

#### **Tham s·ªë ƒëi·ªÅu ch·ªânh:**

| Tham s·ªë | V·ªã tr√≠ | ·∫¢nh h∆∞·ªüng | G·ª£i √Ω ƒëi·ªÅu ch·ªânh |
|---------|--------|-----------|------------------|
| **`min_support`** | `mine_frequent_itemsets()` | **Quan tr·ªçng nh·∫•t**<br>C√†ng th·∫•p ‚Üí nhi·ªÅu lu·∫≠t h∆°n<br>C√†ng cao ‚Üí √≠t lu·∫≠t h∆°n nh∆∞ng m·∫°nh h∆°n | **Baseline**: 0.01 (1%)<br>**Conservative**: 0.02-0.05<br>**Aggressive**: 0.005 |
| **`min_confidence`** | `generate_rules()` | ƒê·ªô tin c·∫≠y t·ªëi thi·ªÉu c·ªßa lu·∫≠t<br>N·∫øu mua A th√¨ % mua B | **Baseline**: 0.3 (30%)<br>**High quality**: 0.5-0.7<br>**Exploratory**: 0.2 |
| **`min_lift`** | `filter_rules()` | Lo·∫°i lu·∫≠t kh√¥ng c√≥ gi√° tr·ªã<br>Lift > 1: A v√† B c√≥ li√™n quan | **Must have**: > 1.0<br>**Good**: > 1.2<br>**Strong**: > 1.5 |
| `max_len` | `mine_frequent_itemsets()` | ƒê·ªô d√†i t·ªëi ƒëa c·ªßa itemset | 2-4 (d·ªÖ di·ªÖn gi·∫£i)<br>5-8 (ph·ª©c t·∫°p h∆°n) |
| `metric` | `generate_rules()` | Metric ƒë·ªÉ sinh lu·∫≠t | 'confidence', 'lift', 'leverage' |

#### **üí° G·ª£i √Ω c·∫£i thi·ªán:**

**Scenario 1: Qu√° √≠t lu·∫≠t (< 50)**
```python
min_support = 0.005  # Gi·∫£m t·ª´ 0.01
min_confidence = 0.2  # Gi·∫£m t·ª´ 0.3
min_lift = 1.0        # Gi·∫£m t·ª´ 1.2
```

**Scenario 2: Qu√° nhi·ªÅu lu·∫≠t (> 1000)**
```python
min_support = 0.02   # TƒÉng t·ª´ 0.01
min_confidence = 0.5 # TƒÉng t·ª´ 0.3
min_lift = 1.5       # TƒÉng t·ª´ 1.2
max_len = 3          # Gi·ªõi h·∫°n ƒë·ªô d√†i
```

**Scenario 3: Ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t**
```python
min_support = 0.01
min_confidence = 0.4
min_lift = 1.3
# + L·ªçc theo antecedent_len >= 2 (√≠t nh·∫•t 2 items)
```

---

### **B∆Ø·ªöC 4: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ lu·∫≠t**
üìÅ **Notebook**: `clustering_from_rules.ipynb`  
üîß **Class**: `RuleBasedCustomerClusterer`

#### **Ch·ª©c nƒÉng:**
Bi·∫øn lu·∫≠t k·∫øt h·ª£p th√†nh vector ƒë·∫∑c tr∆∞ng cho kh√°ch h√†ng:

1. **Load Top-K lu·∫≠t** t·ª´ file rules
2. **Build Customer √ó Item matrix** (boolean)
3. **Build Customer √ó Rule matrix**:
   - M·ªói c·ªôt = 1 lu·∫≠t
   - Gi√° tr·ªã = 1 n·∫øu kh√°ch mua ƒë·ªß antecedents c·ªßa lu·∫≠t, 0 n·∫øu kh√¥ng
   - (Tu·ª≥ ch·ªçn) Nh√¢n tr·ªçng s·ªë theo lift/confidence
4. **Gh√©p RFM** (n·∫øu d√πng)
5. **Chu·∫©n h√≥a** (StandardScaler)

#### **Tham s·ªë ƒëi·ªÅu ch·ªânh:**

| Tham s·ªë | V·ªã tr√≠ | ·∫¢nh h∆∞·ªüng | G·ª£i √Ω |
|---------|--------|-----------|-------|
| **`TOP_K_RULES`** | `load_rules()` | **S·ªë l∆∞·ª£ng lu·∫≠t d√πng l√†m features**<br>C√†ng nhi·ªÅu ‚Üí nhi·ªÅu chi·ªÅu h∆°n | **Baseline**: 200<br>**Small**: 50-100<br>**Large**: 300-500 |
| **`SORT_RULES_BY`** | `load_rules()` | Ti√™u ch√≠ ch·ªçn lu·∫≠t quan tr·ªçng | **'lift'**: ƒê·ªô li√™n quan<br>**'confidence'**: ƒê·ªô tin c·∫≠y<br>**'support'**: ƒê·ªô ph·ªï bi·∫øn |
| **`WEIGHTING`** | `build_rule_feature_matrix()` | **Quan tr·ªçng**<br>C√°ch t√≠nh gi√° tr·ªã feature | **'none'**: 0/1 binary<br>**'lift'**: Nh√¢n lift<br>**'confidence'**: Nh√¢n confidence<br>**'lift_x_conf'**: Lift √ó Confidence |
| **`MIN_ANTECEDENT_LEN`** | `build_rule_feature_matrix()` | L·ªçc lu·∫≠t c√≥ antecedent qu√° ng·∫Øn | **1**: T·∫•t c·∫£ lu·∫≠t<br>**2**: √çt nh·∫•t 2 items<br>**3**: Ph·ª©c t·∫°p h∆°n |
| **`USE_RFM`** | `build_final_features()` | C√≥ gh√©p RFM kh√¥ng? | **True**: Baseline + RFM<br>**False**: Ch·ªâ d√πng rules |
| **`RFM_SCALE`** | `build_final_features()` | Chu·∫©n h√≥a RFM kh√¥ng? | **True**: Khuy·∫øn ngh·ªã<br>**False**: Kh√¥ng scale |
| **`RULE_SCALE`** | `build_final_features()` | Chu·∫©n h√≥a rule features? | **False**: Gi·ªØ nguy√™n 0/1<br>**True**: Scale v·ªÅ [-1, 1] |
| `min_support`<br>`min_confidence`<br>`min_lift` | `load_rules()` | L·ªçc l·∫ßn 2 (sau khi ƒë√£ c√≥ rules) | Tu·ª≥ ch·ªçn, ƒë·ªÉ None n·∫øu ƒë√£ l·ªçc t·ªët ·ªü b∆∞·ªõc 3 |

#### **üí° G·ª£i √Ω c·∫£i thi·ªán:**

**Bi·∫øn th·ªÉ 1: Baseline (Rule-only Binary)**
```python
TOP_K_RULES = 200
WEIGHTING = "none"           # Binary 0/1
USE_RFM = False
MIN_ANTECEDENT_LEN = 1
```

**Bi·∫øn th·ªÉ 2: Rule + RFM**
```python
TOP_K_RULES = 200
WEIGHTING = "none"
USE_RFM = True               # Th√™m RFM
RFM_SCALE = True
```

**Bi·∫øn th·ªÉ 3: Weighted Rules**
```python
TOP_K_RULES = 200
WEIGHTING = "lift_x_conf"    # Tr·ªçng s·ªë k√©p
USE_RFM = False
MIN_ANTECEDENT_LEN = 2       # Ch·ªâ lu·∫≠t ph·ª©c t·∫°p
```

**Bi·∫øn th·ªÉ 4: Full Features (Khuy·∫øn ngh·ªã)**
```python
TOP_K_RULES = 300
WEIGHTING = "lift"           # Tr·ªçng s·ªë lift
USE_RFM = True
RFM_SCALE = True
MIN_ANTECEDENT_LEN = 2
RULE_SCALE = False           # Gi·ªØ nguy√™n ƒë·ªÉ di·ªÖn gi·∫£i
```

**So s√°nh hi·ªáu qu·∫£:**
- **√çt lu·∫≠t + kh√¥ng tr·ªçng s·ªë**: Nhanh, ƒë∆°n gi·∫£n, d·ªÖ di·ªÖn gi·∫£i
- **Nhi·ªÅu lu·∫≠t + tr·ªçng s·ªë**: Ch√≠nh x√°c h∆°n, ph·ª©c t·∫°p h∆°n
- **Rule + RFM**: K·∫øt h·ª£p h√†nh vi mua v√† gi√° tr·ªã kh√°ch h√†ng

---

### **B∆Ø·ªöC 5: Ph√¢n c·ª•m K-Means**
üìÅ **Notebook**: `clustering_from_rules.ipynb`  
üîß **Method**: `choose_k_by_silhouette()`, `fit_kmeans()`

#### **Ch·ª©c nƒÉng:**
1. **Ch·ªçn K t·ªëi ∆∞u**: Th·ª≠ K t·ª´ K_MIN ƒë·∫øn K_MAX, t√≠nh Silhouette Score
2. **Hu·∫•n luy·ªán K-Means**: Fit m√¥ h√¨nh v·ªõi K ƒë√£ ch·ªçn
3. **G√°n nh√£n c·ª•m**: M·ªói kh√°ch h√†ng ƒë∆∞·ª£c g√°n v√†o 1 c·ª•m

#### **Tham s·ªë ƒëi·ªÅu ch·ªânh:**

| Tham s·ªë | V·ªã tr√≠ | ·∫¢nh h∆∞·ªüng | G·ª£i √Ω |
|---------|--------|-----------|-------|
| **`K_MIN`** | `choose_k_by_silhouette()` | S·ªë c·ª•m t·ªëi thi·ªÉu th·ª≠ nghi·ªám | **2** (t·ªëi thi·ªÉu) |
| **`K_MAX`** | `choose_k_by_silhouette()` | S·ªë c·ª•m t·ªëi ƒëa th·ª≠ nghi·ªám | **8-12**<br>(kh√¥ng n√™n qu√° nhi·ªÅu v√¨ kh√≥ di·ªÖn gi·∫£i) |
| **`N_CLUSTERS`** | `fit_kmeans()` | **S·ªë c·ª•m cu·ªëi c√πng**<br>None = t·ª± ƒë·ªông ch·ªçn theo Silhouette | **None**: T·ª± ƒë·ªông<br>**3-6**: Th·ªß c√¥ng (d·ªÖ di·ªÖn gi·∫£i) |
| `RANDOM_STATE` | `fit_kmeans()` | Seed ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£ | 42 (c·ªë ƒë·ªãnh) |
| `n_init` | KMeans parameter | S·ªë l·∫ßn kh·ªüi t·∫°o ng·∫´u nhi√™n | 'auto' ho·∫∑c 10-20 |
| `max_iter` | KMeans parameter | S·ªë v√≤ng l·∫∑p t·ªëi ƒëa | 300 (m·∫∑c ƒë·ªãnh) |

#### **üí° G·ª£i √Ω c·∫£i thi·ªán:**

**Ch·ªçn K theo ng·ªØ c·∫£nh:**
- **K=3-4**: Ph√¢n kh√∫c ƒë∆°n gi·∫£n (VIP, Trung b√¨nh, Th·∫•p)
- **K=5-6**: Ph√¢n kh√∫c chi ti·∫øt (nhi·ªÅu chi·∫øn l∆∞·ª£c h∆°n)
- **K=7+**: Qu√° ph·ª©c t·∫°p, kh√≥ tri·ªÉn khai marketing

**Ph∆∞∆°ng ph√°p ch·ªçn K:**
1. **Silhouette Score** (ƒëang d√πng):
   - Cao nh·∫•t (~0.4-0.6): C·ª•m t√°ch r√µ
   - Trung b√¨nh (0.2-0.4): C·ª•m ch·∫•p nh·∫≠n ƒë∆∞·ª£c
   - Th·∫•p (<0.2): C·ª•m k√©m

2. **Elbow Method** (c√≥ th·ªÉ th√™m):
   ```python
   # V·∫Ω bi·ªÉu ƒë·ªì Inertia (within-cluster sum of squares)
   inertias = []
   for k in range(2, 11):
       km = KMeans(n_clusters=k, random_state=42)
       km.fit(X)
       inertias.append(km.inertia_)
   # T√¨m "khu·ª∑u tay" (elbow) tr√™n ƒë·ªì th·ªã
   ```

3. **Davies-Bouldin Index** (th·∫•p c√†ng t·ªët):
   ```python
   from sklearn.metrics import davies_bouldin_score
   score = davies_bouldin_score(X, labels)
   ```

**So s√°nh thu·∫≠t to√°n kh√°c (N√¢ng cao):**

| Thu·∫≠t to√°n | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | Khi n√†o d√πng |
|------------|---------|------------|--------------|
| **K-Means** | Nhanh, ƒë∆°n gi·∫£n, d·ªÖ di·ªÖn gi·∫£i | Gi·∫£ ƒë·ªãnh c·ª•m tr√≤n, nh·∫°y v·ªõi outliers | **Baseline** (b·∫Øt bu·ªôc) |
| **Agglomerative** | Kh√¥ng c·∫ßn ch·ªçn K tr∆∞·ªõc, ph√¢n c·∫•p | Ch·∫≠m v·ªõi d·ªØ li·ªáu l·ªõn | Mu·ªën dendrogram, ph√¢n c·∫•p kh√°ch h√†ng |
| **DBSCAN** | T√¨m c·ª•m h√¨nh d·∫°ng b·∫•t k·ª≥, t·ª± ƒë·ªông t√¨m outliers | Kh√≥ ch·ªçn epsilon, kh√¥ng ·ªïn ƒë·ªãnh v·ªõi m·∫≠t ƒë·ªô kh√°c nhau | D·ªØ li·ªáu c√≥ nhi·ªÅu noise |
| **HDBSCAN** | C·∫£i ti·∫øn DBSCAN, t·ª± ƒë·ªông h∆°n | Ch·∫≠m h∆°n | Mu·ªën k·∫øt qu·∫£ t·ªët nh·∫•t, kh√¥ng quan t√¢m t·ªëc ƒë·ªô |

---

### **B∆Ø·ªöC 6: Ph√¢n t√≠ch v√† Di·ªÖn gi·∫£i**
üìÅ **Notebook**: `clustering_from_rules.ipynb`  
üîß **Output**: B·∫£ng profiling, tr·ª±c quan h√≥a, chi·∫øn l∆∞·ª£c

#### **Ch·ª©c nƒÉng:**
1. **Profiling c·ª•m**: Th·ªëng k√™ ƒë·∫∑c ƒëi·ªÉm t·ª´ng c·ª•m
2. **Top rules theo c·ª•m**: Lu·∫≠t n√†o ƒë∆∞·ª£c k√≠ch ho·∫°t nhi·ªÅu nh·∫•t
3. **Tr·ª±c quan h√≥a 2D**: Gi·∫£m chi·ªÅu PCA/SVD, v·∫Ω scatter plot
4. **ƒê·∫∑t t√™n v√† chi·∫øn l∆∞·ª£c**: G·∫Øn nh√£n √Ω nghƒ©a cho c·ª•m

#### **Tham s·ªë ƒëi·ªÅu ch·ªânh:**

| Tham s·ªë | V·ªã tr√≠ | ·∫¢nh h∆∞·ªüng | G·ª£i √Ω |
|---------|--------|-----------|-------|
| `PROJECTION_METHOD` | `project_2d()` | Ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu | **'pca'**: Tuy·∫øn t√≠nh<br>**'svd'**: Sparse data |
| `PLOT_2D` | Notebook | C√≥ v·∫Ω scatter plot kh√¥ng | True (khuy·∫øn ngh·ªã) |
| Top rules to show | Custom | S·ªë lu·∫≠t hi·ªÉn th·ªã m·ªói c·ª•m | 5-10 lu·∫≠t |

#### **üí° G·ª£i √Ω ph√¢n t√≠ch:**

**B·∫£ng Profiling m·∫´u:**
```
Cluster | Size | Recency | Frequency | Monetary | Top Rules | T√™n | Chi·∫øn l∆∞·ª£c
--------|------|---------|-----------|----------|-----------|-----|------------
0       | 450  | 15      | 8         | 1200     | Tea‚ÜíMug   | VIP | ChƒÉm s√≥c ri√™ng, ∆∞u ƒë√£i ƒë·∫∑c bi·ªát
1       | 820  | 45      | 3         | 300      | Candle    | Casual | Cross-sell, bundle deals
2       | 210  | 180     | 1         | 150      | Gift      | Dormant | K√≠ch ho·∫°t l·∫°i, gi·∫£m gi√° m·∫°nh
```

**C√°c metric ƒë√°nh gi√°:**
- **Silhouette Score**: 0.3-0.5 l√† t·ªët
- **Cluster size balance**: Kh√¥ng c√≥ c·ª•m qu√° nh·ªè (< 5%) ho·∫∑c qu√° l·ªõn (> 70%)
- **RFM variance**: C√°c c·ª•m c√≥ RFM kh√°c bi·ªát r√µ r·ªát

---

## üéØ CHI·∫æN L∆Ø·ª¢C ƒêI·ªÄU CH·ªàNH THAM S·ªê

### **K·ªãch b·∫£n 1: C·ª•m kh√¥ng t√°ch r√µ (Silhouette < 0.2)**

**Nguy√™n nh√¢n:**
- ƒê·∫∑c tr∆∞ng k√©m ph√¢n bi·ªát
- Qu√° √≠t features
- Kh√¥ng chu·∫©n h√≥a

**Gi·∫£i ph√°p:**
1. TƒÉng `TOP_K_RULES` l√™n 300-500
2. D√πng `WEIGHTING = "lift"` ho·∫∑c `"lift_x_conf"`
3. B·∫≠t `USE_RFM = True` v√† `RFM_SCALE = True`
4. TƒÉng `MIN_ANTECEDENT_LEN = 2` ƒë·ªÉ l·ªçc lu·∫≠t ch·∫•t l∆∞·ª£ng
5. Th·ª≠ gi·∫£m K xu·ªëng 3-4

---

### **K·ªãch b·∫£n 2: Qu√° nhi·ªÅu c·ª•m nh·ªè l·∫ª**

**Nguy√™n nh√¢n:**
- K qu√° l·ªõn
- D·ªØ li·ªáu c√≥ nhi·ªÅu outliers

**Gi·∫£i ph√°p:**
1. Gi·∫£m `K_MAX` xu·ªëng 6-8
2. L·ªçc kh√°ch h√†ng c√≥ `Frequency < 2` tr∆∞·ªõc khi ph√¢n c·ª•m
3. Th·ª≠ DBSCAN ƒë·ªÉ t·ª± ƒë·ªông lo·∫°i outliers

---

### **K·ªãch b·∫£n 3: T·∫•t c·∫£ c·ª•m gi·ªëng nhau**

**Nguy√™n nh√¢n:**
- ƒê·∫∑c tr∆∞ng kh√¥ng ƒëa d·∫°ng
- Lu·∫≠t qu√° ph·ªï bi·∫øn (support cao)

**Gi·∫£i ph√°p:**
1. Gi·∫£m `min_support` ·ªü b∆∞·ªõc 3 ƒë·ªÉ c√≥ lu·∫≠t ƒëa d·∫°ng h∆°n
2. TƒÉng `min_lift` l√™n 1.5 ƒë·ªÉ ch·ªâ l·∫•y lu·∫≠t m·∫°nh
3. S·∫Øp x·∫øp lu·∫≠t theo `confidence` thay v√¨ `lift`
4. TƒÉng `TOP_K_RULES` l√™n 400-500

---

### **K·ªãch b·∫£n 4: C·ª•m kh√¥ng c√≥ √Ω nghƒ©a marketing**

**Nguy√™n nh√¢n:**
- Ph√¢n c·ª•m ch·ªâ d·ª±a v√†o rules, thi·∫øu context gi√° tr·ªã kh√°ch h√†ng

**Gi·∫£i ph√°p:**
1. **B·∫Øt bu·ªôc** b·∫≠t `USE_RFM = True`
2. C√¢n nh·∫Øc tƒÉng t·ª∑ tr·ªçng RFM:
   ```python
   # Nh√¢n RFM v·ªõi tr·ªçng s·ªë l·ªõn h∆°n
   rfm_values = rfm_values * 2  # Ho·∫∑c 3
   ```
3. Th√™m ƒë·∫∑c tr∆∞ng kh√°c: T·ªïng s·ªë ƒë∆°n, Trung b√¨nh gi√° tr·ªã ƒë∆°n

---

## ÔøΩ C√ÅCH TR√åNH B√ÄY L·ª∞A CH·ªåN LU·∫¨T (Y√äU C·∫¶U B·∫ÆT BU·ªòC)

### **M·ª•c ƒë√≠ch:**
Nh√≥m ph·∫£i **gi·∫£i th√≠ch r√µ r√†ng** v√† **minh ch·ª©ng b·∫±ng s·ªë li·ªáu** c√°ch l·ª±a ch·ªçn lu·∫≠t k·∫øt h·ª£p l√†m ƒë·∫ßu v√†o cho ph√¢n c·ª•m.

### **N·ªôi dung c·∫ßn tr√¨nh b√†y:**

#### **1. Gi·∫£i th√≠ch quy·∫øt ƒë·ªãnh l·ª±a ch·ªçn**

Template m·∫´u:

```markdown
### L·ª±a ch·ªçn lu·∫≠t k·∫øt h·ª£p cho Feature Engineering

#### 1.1. Ngu·ªìn d·ªØ li·ªáu lu·∫≠t
- **File s·ª≠ d·ª•ng**: `rules_apriori_filtered.csv` (ho·∫∑c `rules_fpgrowth_filtered.csv`)
- **T·ªïng s·ªë lu·∫≠t ban ƒë·∫ßu**: 1,234 lu·∫≠t
- **Thu·∫≠t to√°n**: Apriori (ho·∫∑c FP-Growth)

#### 1.2. Ti√™u ch√≠ ch·ªçn Top-K lu·∫≠t
- **Top-K**: Ch·ªçn 200 lu·∫≠t h√†ng ƒë·∫ßu
- **L√Ω do ch·ªçn K=200**: 
  - ƒê·ªß l·ªõn ƒë·ªÉ capture ƒë∆∞·ª£c ƒëa d·∫°ng h√†nh vi mua s·∫Øm
  - Kh√¥ng qu√° nhi·ªÅu tr√°nh overfitting v√† chi·ªÅu cao
  - Th·ª≠ nghi·ªám v·ªõi K=100, 200, 300 cho th·∫•y K=200 cho Silhouette score t·ªët nh·∫•t

#### 1.3. Ti√™u ch√≠ s·∫Øp x·∫øp
- **S·∫Øp x·∫øp theo**: `lift` (gi·∫£m d·∫ßn)
- **L√Ω do**: 
  - Lift ƒëo ƒë·ªô li√™n quan gi·ªØa antecedent v√† consequent
  - Lift > 1 nghƒ©a l√† mua A l√†m tƒÉng x√°c su·∫•t mua B
  - ∆Øu ti√™n lu·∫≠t c√≥ lift cao ƒë·ªÉ t·∫°o features ph√¢n bi·ªát r√µ r√†ng gi·ªØa c√°c nh√≥m kh√°ch h√†ng

**Alternative**: C√≥ th·ªÉ s·∫Øp x·∫øp theo `confidence` n·∫øu mu·ªën ∆∞u ti√™n ƒë·ªô tin c·∫≠y

#### 1.4. Ng∆∞·ª°ng l·ªçc b·ªï sung
- **min_support**: 0.01 (1%) - Ch·ªâ gi·ªØ lu·∫≠t xu·∫•t hi·ªán √≠t nh·∫•t 1% baskets
- **min_confidence**: 0.3 (30%) - ƒê·ªô tin c·∫≠y t·ªëi thi·ªÉu
- **min_lift**: 1.2 - Ch·ªâ gi·ªØ lu·∫≠t c√≥ t∆∞∆°ng quan d∆∞∆°ng m·∫°nh
- **min_antecedent_len**: 2 - Ch·ªâ gi·ªØ lu·∫≠t c√≥ √≠t nh·∫•t 2 items trong antecedent (lo·∫°i lu·∫≠t ƒë∆°n gi·∫£n)

#### 1.5. L√Ω do ch·ªçn b·ªô ng∆∞·ª°ng n√†y
- Support 1%: ƒê·∫£m b·∫£o lu·∫≠t ƒë·ªß ph·ªï bi·∫øn, kh√¥ng ph·∫£i noise
- Confidence 30%: C√¢n b·∫±ng gi·ªØa s·ªë l∆∞·ª£ng v√† ch·∫•t l∆∞·ª£ng lu·∫≠t
- Lift 1.2: Ch·ªâ l·∫•y lu·∫≠t c√≥ √Ω nghƒ©a th·ªëng k√™ (lift c√†ng cao c√†ng t·ªët)
- Antecedent ‚â• 2: Lu·∫≠t ph·ª©c t·∫°p h∆°n gi√∫p ph√¢n bi·ªát h√†nh vi mua k√®m
```

---

#### **2. B·∫£ng 10 lu·∫≠t ti√™u bi·ªÉu**

**Code ƒë·ªÉ tr√≠ch xu·∫•t:**

```python
# Sau khi load rules
clusterer = RuleBasedCustomerClusterer(df_clean)
clusterer.build_customer_item_matrix()
rules_top = clusterer.load_rules(
    rules_csv_path="data/processed/rules_apriori_filtered.csv",
    top_k=200,
    sort_by="lift",
    min_support=0.01,
    min_confidence=0.3,
    min_lift=1.2
)

# Hi·ªÉn th·ªã 10 lu·∫≠t ti√™u bi·ªÉu
print("### Top 10 lu·∫≠t ƒë∆∞·ª£c ch·ªçn l√†m ƒë·∫ßu v√†o:")
display_cols = ['antecedents_str', 'consequents_str', 'support', 'confidence', 'lift']
print(rules_top.head(10)[display_cols].to_markdown(index=True))
```

**K·∫øt qu·∫£ m·∫´u:**

| # | Antecedents | Consequents | Support | Confidence | Lift |
|---|-------------|-------------|---------|------------|------|
| 1 | REGENCY CAKESTAND 3 TIER, PINK REGENCY TEACUP AND SAUCER | GREEN REGENCY TEACUP AND SAUCER | 0.0156 | 0.7692 | 15.38 |
| 2 | GREEN REGENCY TEACUP AND SAUCER, ROSES REGENCY TEACUP AND SAUCER | PINK REGENCY TEACUP AND SAUCER | 0.0134 | 0.7500 | 14.42 |
| 3 | JUMBO BAG RED RETROSPOT, LUNCH BAG RED RETROSPOT | CHARLOTTE BAG PINK POLKADOT | 0.0112 | 0.6923 | 12.85 |
| 4 | SET/6 RED SPOTTY PAPER CUPS, SET/6 RED SPOTTY PAPER PLATES | SET/20 RED RETROSPOT PAPER NAPKINS | 0.0145 | 0.8125 | 11.24 |
| 5 | PARTY BUNTING, POPCORN HOLDER | PAPER CHAIN KIT 50'S CHRISTMAS | 0.0098 | 0.6531 | 10.85 |
| 6 | PLASTERS IN TIN CIRCUS PARADE, PLASTERS IN TIN WOODLAND ANIMALS | PLASTERS IN TIN SPACEBOY | 0.0087 | 0.6154 | 9.73 |
| 7 | FELTCRAFT PRINCESS CHARLOTTE DOLL, MINI CAKE STAND 2 TIER | ALARM CLOCK BAKELIKE PINK | 0.0076 | 0.5833 | 8.92 |
| 8 | GARDENERS KNEELING PAD CUP OF TEA, GARDENERS KNEELING PAD KEEP CALM | GARDENERS KNEELING PAD RETROSPOT | 0.0123 | 0.7241 | 8.45 |
| 9 | PACK OF 72 RETROSPOT CAKE CASES, SAVE THE PLANET MUG | RECIPE BOX PANTRY YELLOW DESIGN | 0.0065 | 0.5417 | 7.82 |
| 10 | DOORMAT NEW ENGLAND, WOOD 2 DRAWER CABINET WHITE FINISH | WOOD S/3 CABINET ANT WHITE FINISH | 0.0054 | 0.5000 | 7.14 |

**Nh·∫≠n x√©t v·ªÅ ch·∫•t l∆∞·ª£ng:**
- **Lift**: T·∫•t c·∫£ > 7.0, ch·ª©ng t·ªè t∆∞∆°ng quan r·∫•t m·∫°nh gi·ªØa c√°c s·∫£n ph·∫©m
- **Confidence**: Dao ƒë·ªông 50-81%, ƒë·ªß tin c·∫≠y ƒë·ªÉ l√†m features
- **Support**: T·ª´ 0.5% ƒë·∫øn 1.6%, ƒë·∫£m b·∫£o kh√¥ng qu√° ph·ªï bi·∫øn (universal) c≈©ng kh√¥ng qu√° hi·∫øm (noise)
- **√ù nghƒ©a kinh doanh**: C√°c lu·∫≠t ph·∫£n √°nh c√°c nh√≥m s·∫£n ph·∫©m:
  - Nh√≥m 1-2: B·ªô t√°ch tr√† Regency (kh√°ch h√†ng mua nhi·ªÅu m√†u)
  - Nh√≥m 3-4: ƒê·ªì d√πng ti·ªác (Red Retrospot collection)
  - Nh√≥m 5: Trang tr√≠ ti·ªác
  - Nh√≥m 6: BƒÉng keo c√° nh√¢n (tr·∫ª em)
  - Nh√≥m 7-10: ƒê·ªì gia d·ª•ng, trang tr√≠ nh√†

---

#### **3. Ph√¢n t√≠ch ph√¢n b·ªë lu·∫≠t**

```python
# Th·ªëng k√™ t·ªïng quan
print(f"S·ªë lu·∫≠t sau khi l·ªçc: {len(rules_top)}")
print(f"\nPh√¢n b·ªë Support:")
print(rules_top['support'].describe())
print(f"\nPh√¢n b·ªë Confidence:")
print(rules_top['confidence'].describe())
print(f"\nPh√¢n b·ªë Lift:")
print(rules_top['lift'].describe())

# Visualize
import matplotlib.pyplot as plt
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
rules_top['support'].hist(bins=30, ax=axes[0])
axes[0].set_title('Distribution of Support')
rules_top['confidence'].hist(bins=30, ax=axes[1])
axes[1].set_title('Distribution of Confidence')
rules_top['lift'].hist(bins=30, ax=axes[2])
axes[2].set_title('Distribution of Lift')
plt.tight_layout()
plt.show()
```

**Output m·∫´u:**
```
S·ªë lu·∫≠t sau khi l·ªçc: 200

Ph√¢n b·ªë Support:
count    200.000
mean       0.015
std        0.008
min        0.010
25%        0.011
50%        0.013
75%        0.017
max        0.045

Ph√¢n b·ªë Confidence:
count    200.000
mean       0.52
std        0.14
min        0.30
25%        0.42
50%        0.51
75%        0.63
max        0.85

Ph√¢n b·ªë Lift:
count    200.000
mean       5.8
std        3.2
min        1.2
25%        3.4
50%        4.9
75%        7.1
max       18.5
```

**Nh·∫≠n x√©t:**
- Support t·∫≠p trung ·ªü 1-2%, ph√π h·ª£p v·ªõi long-tail products
- Confidence trung b√¨nh 52%, cho th·∫•y lu·∫≠t c√≥ ƒë·ªô tin c·∫≠y v·ª´a ph·∫£i
- Lift trung b√¨nh 5.8, cho th·∫•y t∆∞∆°ng quan m·∫°nh (>> 1.0)

---

#### **4. So s√°nh c√°c ph∆∞∆°ng √°n l·ª±a ch·ªçn**

| Ph∆∞∆°ng √°n | Top-K | Sort by | min_lift | S·ªë lu·∫≠t cu·ªëi | Silhouette | Nh·∫≠n x√©t |
|-----------|-------|---------|----------|--------------|------------|----------|
| **A (Baseline)** | 200 | lift | 1.0 | 200 | 0.34 | Baseline t·ªët |
| **B (Conservative)** | 150 | lift | 1.5 | 150 | 0.37 | ‚úÖ T·ªët nh·∫•t - ch·ªâ l·∫•y lu·∫≠t m·∫°nh |
| **C (Aggressive)** | 300 | lift | 1.2 | 300 | 0.31 | Qu√° nhi·ªÅu features, overfitting |
| **D (Confidence-based)** | 200 | confidence | 1.2 | 200 | 0.33 | T∆∞∆°ng ƒë∆∞∆°ng baseline |

**K·∫øt lu·∫≠n**: Ch·ªçn ph∆∞∆°ng √°n B v·ªõi Top-150 lu·∫≠t c√≥ lift ‚â• 1.5

---

### **Template code ho√†n ch·ªânh cho notebook:**

```python
# Cell: Gi·∫£i th√≠ch l·ª±a ch·ªçn lu·∫≠t
print("="*80)
print("PH·∫¶N 1: L·ª∞A CH·ªåN LU·∫¨T K·∫æT H·ª¢P CHO FEATURE ENGINEERING")
print("="*80)

print("\n### 1. Ngu·ªìn d·ªØ li·ªáu")
print(f"- File: {RULES_INPUT_PATH}")
rules_raw = pd.read_csv(RULES_INPUT_PATH)
print(f"- T·ªïng s·ªë lu·∫≠t ban ƒë·∫ßu: {len(rules_raw):,}")

print("\n### 2. Ti√™u ch√≠ l·ª±a ch·ªçn")
print(f"- Top-K: {TOP_K_RULES}")
print(f"- S·∫Øp x·∫øp theo: {SORT_RULES_BY}")
print(f"- Ng∆∞·ª°ng: min_support={MIN_SUPPORT}, min_confidence={MIN_CONFIDENCE}, min_lift={MIN_LIFT}")
print(f"- ƒê·ªô d√†i antecedent t·ªëi thi·ªÉu: {MIN_ANTECEDENT_LEN}")

print("\n### 3. L√Ω do l·ª±a ch·ªçn")
print("""
- TOP_K=200: ƒê·ªß l·ªõn ƒë·ªÉ capture h√†nh vi ƒëa d·∫°ng, kh√¥ng qu√° nhi·ªÅu tr√°nh overfitting
- Sort by lift: ∆Øu ti√™n lu·∫≠t c√≥ t∆∞∆°ng quan m·∫°nh nh·∫•t
- min_lift=1.2: Ch·ªâ gi·ªØ lu·∫≠t c√≥ √Ω nghƒ©a th·ªëng k√™
- min_antecedent_len=2: L·ªçc lu·∫≠t ph·ª©c t·∫°p, lo·∫°i lu·∫≠t ƒë∆°n gi·∫£n A‚ÜíB
""")

# Cell: Load v√† hi·ªÉn th·ªã 10 lu·∫≠t ti√™u bi·ªÉu
clusterer = RuleBasedCustomerClusterer(df_clean)
clusterer.build_customer_item_matrix()
rules_top = clusterer.load_rules(
    rules_csv_path=RULES_INPUT_PATH,
    top_k=TOP_K_RULES,
    sort_by=SORT_RULES_BY,
    min_support=MIN_SUPPORT if 'MIN_SUPPORT' in globals() else None,
    min_confidence=MIN_CONFIDENCE if 'MIN_CONFIDENCE' in globals() else None,
    min_lift=MIN_LIFT if 'MIN_LIFT' in globals() else None,
)

print("\n### 4. Top 10 lu·∫≠t ti√™u bi·ªÉu:")
display_cols = ['antecedents_str', 'consequents_str', 'support', 'confidence', 'lift']
print(rules_top.head(10)[display_cols].to_markdown(index=True, floatfmt=".4f"))

# Cell: Th·ªëng k√™ ph√¢n b·ªë
print("\n### 5. Ph√¢n b·ªë c√°c ch·ªâ s·ªë:")
print("\nSupport:")
print(rules_top['support'].describe())
print("\nConfidence:")
print(rules_top['confidence'].describe())
print("\nLift:")
print(rules_top['lift'].describe())

# Cell: Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 4))
rules_top['support'].hist(bins=30, ax=axes[0], edgecolor='black')
axes[0].set_title('Distribution of Support', fontsize=12, fontweight='bold')
axes[0].set_xlabel('Support')
axes[0].axvline(rules_top['support'].mean(), color='red', linestyle='--', label=f"Mean={rules_top['support'].mean():.4f}")
axes[0].legend()

rules_top['confidence'].hist(bins=30, ax=axes[1], edgecolor='black', color='orange')
axes[1].set_title('Distribution of Confidence', fontsize=12, fontweight='bold')
axes[1].set_xlabel('Confidence')
axes[1].axvline(rules_top['confidence'].mean(), color='red', linestyle='--', label=f"Mean={rules_top['confidence'].mean():.4f}")
axes[1].legend()

rules_top['lift'].hist(bins=30, ax=axes[2], edgecolor='black', color='green')
axes[2].set_title('Distribution of Lift', fontsize=12, fontweight='bold')
axes[2].set_xlabel('Lift')
axes[2].axvline(rules_top['lift'].mean(), color='red', linestyle='--', label=f"Mean={rules_top['lift'].mean():.4f}")
axes[2].legend()

plt.tight_layout()
plt.savefig('figures/rules_distribution.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n‚úÖ ƒê√£ ho√†n th√†nh ph·∫ßn tr√¨nh b√†y l·ª±a ch·ªçn lu·∫≠t!")
```

---

## üìà ROADMAP TH·ª∞C HI·ªÜN

### **Phase 1: Baseline (B·∫Øt bu·ªôc)**
1. ‚úÖ **Tr√¨nh b√†y l·ª±a ch·ªçn lu·∫≠t** (template ·ªü tr√™n)
2. ‚úÖ Ch·∫°y v·ªõi tham s·ªë m·∫∑c ƒë·ªãnh
3. ‚úÖ So s√°nh 2 bi·∫øn th·ªÉ:
   - Rule-only binary
   - Rule + RFM
4. ‚úÖ Ch·ªçn K b·∫±ng Silhouette
5. ‚úÖ Profiling v√† ƒë·∫∑t t√™n c·ª•m

### **Phase 2: Optimization**
1. Th·ª≠ 2-3 c·∫•u h√¨nh kh√°c nhau c·ªßa tham s·ªë
2. So s√°nh b·∫±ng b·∫£ng t·ªïng h·ª£p
3. Ch·ªçn c·∫•u h√¨nh t·ªët nh·∫•t

### **Phase 3: Advanced (N√¢ng cao - Kh√¥ng b·∫Øt bu·ªôc)**
1. **So s√°nh Apriori vs FP-Growth**: N·∫øu ch∆∞a l√†m, th·ª≠ c·∫£ 2 thu·∫≠t to√°n v√† so s√°nh t·ªëc ƒë·ªô, s·ªë l∆∞·ª£ng lu·∫≠t
2. So s√°nh v·ªõi Agglomerative/DBSCAN
3. Th·ª≠ basket clustering ho·∫∑c product clustering
4. X√¢y d·ª±ng Streamlit dashboard

---

## üìù CHECKLIST KI·ªÇM TRA

### **B·∫Øt bu·ªôc:**
- [ ] **‚úÖ Tr√¨nh b√†y r√µ c√°ch ch·ªçn lu·∫≠t**: Top-K, sort_by, ng∆∞·ª°ng l·ªçc, l√Ω do
- [ ] **‚úÖ B·∫£ng 10 lu·∫≠t ti√™u bi·ªÉu**: C√≥ ƒë·∫ßy ƒë·ªß support, confidence, lift
- [ ] **‚úÖ Ph√¢n t√≠ch ph√¢n b·ªë lu·∫≠t**: Histogram c·ªßa support/confidence/lift
- [ ] C√≥ √≠t nh·∫•t 2 bi·∫øn th·ªÉ feature engineering
- [ ] Silhouette score > 0.25
- [ ] M·ªói c·ª•m c√≥ √≠t nh·∫•t 5% t·ªïng kh√°ch h√†ng
- [ ] C√≥ b·∫£ng profiling ƒë·∫ßy ƒë·ªß (size, RFM, top rules)
- [ ] M·ªói c·ª•m c√≥ t√™n v√† chi·∫øn l∆∞·ª£c c·ª• th·ªÉ
- [ ] C√≥ tr·ª±c quan h√≥a 2D
- [ ] C√≥ so s√°nh c√°c bi·∫øn th·ªÉ b·∫±ng b·∫£ng

### **N√¢ng cao (ƒëi·ªÉm t·ªëi ƒëa):**
- [ ] C√≥ so s√°nh thu·∫≠t to√°n kh√°c (Agglomerative/DBSCAN)
- [ ] C√≥ dashboard Streamlit
- [ ] Th·ª≠ basket/product/rule clustering

---

## üéì H∆Ø·ªöNG D·∫™N TH·ª∞C HI·ªÜN CHI TI·∫æT (STEP-BY-STEP)

Ph·∫ßn n√†y gi·∫£i th√≠ch **t·ª´ng b∆∞·ªõc c·ª• th·ªÉ** v·ªõi **thu·∫≠t ng·ªØ chuy√™n ng√†nh** v√† **v·ªã tr√≠ trong h·ªá th·ªëng**.

---

### **B∆Ø·ªöC 1: Ch·∫°y Notebook Clustering v·ªõi C√°c Bi·∫øn Th·ªÉ**

#### **üìö Gi·∫£i th√≠ch thu·∫≠t ng·ªØ:**

| Thu·∫≠t ng·ªØ | Gi·∫£i th√≠ch | V√≠ d·ª• |
|-----------|------------|-------|
| **Feature Engineering** | Qu√° tr√¨nh t·∫°o ra c√°c bi·∫øn ƒë·∫ßu v√†o (features/ƒë·∫∑c tr∆∞ng) cho m√¥ h√¨nh ML t·ª´ d·ªØ li·ªáu th√¥ | T·ª´ lu·∫≠t "A‚ÜíB" t·∫°o ra feature: kh√°ch c√≥ mua A kh√¥ng? |
| **Bi·∫øn th·ªÉ (Variant)** | C√°c c√°ch kh√°c nhau ƒë·ªÉ x√¢y d·ª±ng features, kh√°c nhau v·ªÅ tham s·ªë ho·∫∑c ph∆∞∆°ng ph√°p | Bi·∫øn th·ªÉ 1: ch·ªâ d√πng rules; Bi·∫øn th·ªÉ 2: rules + RFM |
| **Top-K Rules** | L·∫•y K lu·∫≠t t·ªët nh·∫•t (theo lift/confidence) ƒë·ªÉ l√†m features | Top-200 = l·∫•y 200 lu·∫≠t c√≥ lift cao nh·∫•t |
| **Weighting** | Ph∆∞∆°ng ph√°p g√°n tr·ªçng s·ªë cho feature thay v√¨ ch·ªâ 0/1 | Binary (0/1) vs Weighted (nh√¢n lift) |
| **RFM** | Recency-Frequency-Monetary: ƒëo l∆∞·ªùng gi√° tr·ªã kh√°ch h√†ng | R=15 (mua 15 ng√†y tr∆∞·ªõc), F=8 (8 ƒë∆°n), M=1200 (t·ªïng chi 1200$) |

#### **üìç V·ªã tr√≠ trong h·ªá th·ªëng:**

- **File ch√≠nh**: `notebooks/clustering_from_rules.ipynb` (g·ªëc) ho·∫∑c `notebooks/runs/clustering_from_rules_run.ipynb` (ƒë√£ ch·∫°y)
- **File config**: `run_papermill.py` (d√≤ng 125-150)
- **Class x·ª≠ l√Ω**: `src/cluster_library.py` ‚Üí `RuleBasedCustomerClusterer`
- **D·ªØ li·ªáu ƒë·∫ßu v√†o**: 
  - `data/processed/cleaned_uk_data.csv`
  - `data/processed/rules_fpgrowth_filtered.csv`
- **D·ªØ li·ªáu ƒë·∫ßu ra**: `data/processed/customer_clusters_from_rules.csv`

#### **üîß H∆∞·ªõng d·∫´n th·ª±c hi·ªán:**

**B∆∞·ªõc 1.1: T·∫°o notebook m·ªõi cho t·ª´ng bi·∫øn th·ªÉ**

```bash
# T·∫°o b·∫£n sao notebook cho c√°c bi·∫øn th·ªÉ
cd notebooks
cp clustering_from_rules.ipynb clustering_variant_1_baseline.ipynb
cp clustering_from_rules.ipynb clustering_variant_2_rfm.ipynb
cp clustering_from_rules.ipynb clustering_variant_3_weighted.ipynb
```

**B∆∞·ªõc 1.2: C·∫•u h√¨nh t·ª´ng bi·∫øn th·ªÉ**

**Bi·∫øn th·ªÉ 1: Baseline (Rule-only Binary)**
```python
# Cell Parameters trong clustering_variant_1_baseline.ipynb
TOP_K_RULES = 200
SORT_RULES_BY = "lift"
WEIGHTING = "none"              # Binary 0/1
MIN_ANTECEDENT_LEN = 1
USE_RFM = False                 # KH√îNG d√πng RFM
RFM_SCALE = False
RULE_SCALE = False

K_MIN = 3
K_MAX = 8
N_CLUSTERS = None               # T·ª± ƒë·ªông ch·ªçn b·∫±ng Silhouette
```

**Bi·∫øn th·ªÉ 2: Rules + RFM**
```python
# Cell Parameters trong clustering_variant_2_rfm.ipynb
TOP_K_RULES = 200
SORT_RULES_BY = "lift"
WEIGHTING = "none"
MIN_ANTECEDENT_LEN = 1
USE_RFM = True                  # TH√äM RFM
RFM_SCALE = True                # Chu·∫©n h√≥a RFM
RULE_SCALE = False

K_MIN = 3
K_MAX = 8
N_CLUSTERS = None
```

**Bi·∫øn th·ªÉ 3: Weighted Rules + RFM**
```python
# Cell Parameters trong clustering_variant_3_weighted.ipynb
TOP_K_RULES = 150               # √çt h∆°n nh∆∞ng ch·ªçn l·ªçc h∆°n
SORT_RULES_BY = "lift"
WEIGHTING = "lift"              # Nh√¢n tr·ªçng s·ªë lift
MIN_ANTECEDENT_LEN = 2          # Ch·ªâ l·∫•y lu·∫≠t ph·ª©c t·∫°p (‚â•2 items)
USE_RFM = True
RFM_SCALE = True
RULE_SCALE = False

K_MIN = 3
K_MAX = 8
N_CLUSTERS = None
```

**B∆∞·ªõc 1.3: Ch·∫°y t·ª´ng bi·∫øn th·ªÉ**

```python
# Ch·∫°y trong Jupyter ho·∫∑c VS Code
# M·ªü t·ª´ng notebook v√† ch·∫°y t·∫•t c·∫£ cells (Run All)
# Ho·∫∑c d√πng papermill:

import papermill as pm

variants = [
    ("clustering_variant_1_baseline.ipynb", {"USE_RFM": False, "WEIGHTING": "none"}),
    ("clustering_variant_2_rfm.ipynb", {"USE_RFM": True, "WEIGHTING": "none"}),
    ("clustering_variant_3_weighted.ipynb", {"USE_RFM": True, "WEIGHTING": "lift"}),
]

for nb_name, params in variants:
    pm.execute_notebook(
        f"notebooks/{nb_name}",
        f"notebooks/runs/{nb_name}",
        parameters=params,
        kernel_name="python3"
    )
```

**B∆∞·ªõc 1.4: Ghi l·∫°i k·∫øt qu·∫£**

T·∫°o b·∫£ng so s√°nh trong notebook ho·∫∑c file Excel:

| Bi·∫øn th·ªÉ | TOP_K | Weighting | USE_RFM | MIN_ANT_LEN | K (ch·ªçn) | Silhouette | Inertia | Th·ªùi gian | Ghi ch√∫ |
|----------|-------|-----------|---------|-------------|----------|------------|---------|-----------|---------|
| 1 - Baseline | 200 | none | False | 1 | 5 | 0.32 | 15432 | 10s | Baseline ƒë∆°n gi·∫£n |
| 2 - RFM | 200 | none | True | 1 | 4 | 0.38 | 12890 | 12s | ‚úÖ T·ªët h∆°n baseline |
| 3 - Weighted | 150 | lift | True | 2 | 5 | 0.41 | 11234 | 15s | üèÜ T·ªët nh·∫•t |

---

### **B∆Ø·ªöC 2: Ph√¢n T√≠ch v√† Profiling T·ª´ng C·ª•m**

#### **üìö Gi·∫£i th√≠ch thu·∫≠t ng·ªØ:**

| Thu·∫≠t ng·ªØ | Gi·∫£i th√≠ch | M·ª•c ƒë√≠ch |
|-----------|------------|----------|
| **Cluster Profiling** | M√¥ t·∫£ ƒë·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng c·ª•m b·∫±ng th·ªëng k√™ t·ªïng h·ª£p | Hi·ªÉu "c·ª•m n√†y l√† ai?" |
| **Centroid** | ƒêi·ªÉm trung t√¢m c·ªßa c·ª•m (trung b√¨nh c√°c feature) | ƒê·∫°i di·ªán cho c·ª•m |
| **Within-cluster variance** | ƒê·ªô ph√¢n t√°n trong c·ª•m (m·ª©c ƒë·ªô ƒë·ªìng nh·∫•t) | C·ª•m c√†ng "ch·∫∑t" c√†ng t·ªët |
| **Between-cluster variance** | ƒê·ªô kh√°c bi·ªát gi·ªØa c√°c c·ª•m | C·ª•m c√†ng "t√°ch r·ªùi" c√†ng t·ªët |
| **Top Rules per Cluster** | C√°c lu·∫≠t ƒë∆∞·ª£c k√≠ch ho·∫°t nhi·ªÅu nh·∫•t trong c·ª•m | H√†nh vi ƒë·∫∑c tr∆∞ng c·ªßa c·ª•m |
| **Persona** | M√¥ t·∫£ nh√¢n v·∫≠t ƒë·∫°i di·ªán cho c·ª•m | V√≠ d·ª•: "B√† n·ªôi tr·ª£ th√≠ch ƒë·ªì b·∫øp" |

#### **üìç V·ªã tr√≠ trong h·ªá th·ªëng:**

Th√™m v√†o cu·ªëi notebook `clustering_from_rules.ipynb` ho·∫∑c t·∫°o notebook m·ªõi `clustering_profiling.ipynb`

#### **üîß H∆∞·ªõng d·∫´n th·ª±c hi·ªán:**

**B∆∞·ªõc 2.1: Load k·∫øt qu·∫£ ph√¢n c·ª•m**

```python
# Cell: Load d·ªØ li·ªáu
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load k·∫øt qu·∫£
clusters_df = pd.read_csv("data/processed/customer_clusters_from_rules.csv")
rules_df = pd.read_csv("data/processed/rules_fpgrowth_filtered.csv")
df_clean = pd.read_csv("data/processed/cleaned_uk_data.csv")

print(f"S·ªë kh√°ch h√†ng: {len(clusters_df)}")
print(f"S·ªë c·ª•m: {clusters_df['cluster'].nunique()}")
```

**B∆∞·ªõc 2.2: Th·ªëng k√™ c∆° b·∫£n theo c·ª•m**

```python
# Cell: Th·ªëng k√™ t·ªïng quan
profile = clusters_df.groupby('cluster').agg({
    'CustomerID': 'count',        # S·ªë l∆∞·ª£ng
    'Recency': ['mean', 'median'],
    'Frequency': ['mean', 'median'],
    'Monetary': ['mean', 'median']
}).round(2)

profile.columns = ['Size', 'Recency_Mean', 'Recency_Median', 
                   'Frequency_Mean', 'Frequency_Median',
                   'Monetary_Mean', 'Monetary_Median']
profile['Percentage'] = (profile['Size'] / len(clusters_df) * 100).round(2)

print("="*80)
print("B·∫¢NG PROFILING C∆† B·∫¢N")
print("="*80)
print(profile.to_string())

# V·∫Ω bi·ªÉu ƒë·ªì
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# K√≠ch th∆∞·ªõc c·ª•m
profile['Size'].plot(kind='bar', ax=axes[0,0], color='steelblue')
axes[0,0].set_title('Cluster Size')
axes[0,0].set_ylabel('Number of Customers')

# RFM theo c·ª•m
profile[['Recency_Mean', 'Frequency_Mean']].plot(kind='bar', ax=axes[0,1])
axes[0,1].set_title('Average RFM by Cluster')

profile['Monetary_Mean'].plot(kind='bar', ax=axes[1,0], color='green')
axes[1,0].set_title('Average Monetary by Cluster')

# T·ª∑ l·ªá %
profile['Percentage'].plot(kind='pie', ax=axes[1,1], autopct='%1.1f%%')
axes[1,1].set_title('Cluster Distribution')

plt.tight_layout()
plt.savefig('figures/cluster_profiling_basic.png', dpi=150)
plt.show()
```

**B∆∞·ªõc 2.3: T√¨m Top Rules cho t·ª´ng c·ª•m**

```python
# Cell: Top Rules per Cluster
from cluster_library import RuleBasedCustomerClusterer

# Rebuild feature matrix ƒë·ªÉ bi·∫øt rule n√†o ƒë∆∞·ª£c k√≠ch ho·∫°t
clusterer = RuleBasedCustomerClusterer(df_clean)
clusterer.build_customer_item_matrix()
rules_top = clusterer.load_rules("data/processed/rules_fpgrowth_filtered.csv", top_k=200)
X_rules = clusterer.build_rule_feature_matrix(weighting="none", min_antecedent_len=1)

# T·∫°o DataFrame rule activation
rule_activation = pd.DataFrame(
    X_rules, 
    columns=[f"rule_{i}" for i in range(X_rules.shape[1])],
    index=clusterer.customers_
)
rule_activation['cluster'] = clusters_df.set_index('CustomerID')['cluster']

# T√≠nh t·ª∑ l·ªá k√≠ch ho·∫°t m·ªói rule trong t·ª´ng c·ª•m
print("="*80)
print("TOP 10 LU·∫¨T THEO T·ª™NG C·ª§M")
print("="*80)

for cluster_id in sorted(clusters_df['cluster'].unique()):
    cluster_data = rule_activation[rule_activation['cluster'] == cluster_id]
    
    # T·ª∑ l·ªá k√≠ch ho·∫°t
    activation_rates = cluster_data.drop('cluster', axis=1).mean().sort_values(ascending=False).head(10)
    
    print(f"\nüîπ CLUSTER {cluster_id} (n={len(cluster_data)}):")
    print("-" * 80)
    
    for i, (rule_col, rate) in enumerate(activation_rates.items(), 1):
        rule_idx = int(rule_col.split('_')[1])
        rule_row = rules_top.iloc[rule_idx]
        
        print(f"{i}. [{rate*100:.1f}%] {rule_row['antecedents_str']} ‚Üí {rule_row['consequents_str']}")
        print(f"   Support: {rule_row['support']:.3f}, Confidence: {rule_row['confidence']:.3f}, Lift: {rule_row['lift']:.2f}")
```

**B∆∞·ªõc 2.4: Ph√¢n t√≠ch s√¢u h∆°n**

```python
# Cell: Chi ti·∫øt t·ª´ng c·ª•m
for cluster_id in sorted(clusters_df['cluster'].unique()):
    cluster_customers = clusters_df[clusters_df['cluster'] == cluster_id]
    
    print("\n" + "="*80)
    print(f"PH√ÇN T√çCH CHI TI·∫æT CLUSTER {cluster_id}")
    print("="*80)
    
    # 1. Th·ªëng k√™ RFM
    print("\nüìä Th·ªëng k√™ RFM:")
    print(cluster_customers[['Recency', 'Frequency', 'Monetary']].describe())
    
    # 2. Ph√¢n b·ªë RFM
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    cluster_customers['Recency'].hist(bins=30, ax=axes[0], edgecolor='black')
    axes[0].set_title(f'Cluster {cluster_id} - Recency Distribution')
    cluster_customers['Frequency'].hist(bins=30, ax=axes[1], edgecolor='black')
    axes[1].set_title(f'Cluster {cluster_id} - Frequency Distribution')
    cluster_customers['Monetary'].hist(bins=30, ax=axes[2], edgecolor='black')
    axes[2].set_title(f'Cluster {cluster_id} - Monetary Distribution')
    plt.tight_layout()
    plt.savefig(f'figures/cluster_{cluster_id}_rfm_dist.png', dpi=150)
    plt.show()
    
    # 3. S·∫£n ph·∫©m ph·ªï bi·∫øn nh·∫•t
    customer_ids = cluster_customers['CustomerID'].values
    cluster_transactions = df_clean[df_clean['CustomerID'].isin(customer_ids)]
    top_products = cluster_transactions['Description'].value_counts().head(10)
    
    print(f"\nüõçÔ∏è Top 10 s·∫£n ph·∫©m ƒë∆∞·ª£c mua nhi·ªÅu nh·∫•t:")
    for i, (product, count) in enumerate(top_products.items(), 1):
        print(f"  {i}. {product}: {count} l·∫ßn")
```

---

### **B∆Ø·ªöC 3: ƒê·∫∑t T√™n C·ª•m v√† Chi·∫øn L∆∞·ª£c Marketing**

#### **üìö Gi·∫£i th√≠ch thu·∫≠t ng·ªØ:**

| Thu·∫≠t ng·ªØ | Gi·∫£i th√≠ch | V√≠ d·ª• |
|-----------|------------|-------|
| **Segment Naming** | ƒê·∫∑t t√™n c√≥ √Ω nghƒ©a cho c·ª•m thay v√¨ s·ªë | Cluster 0 ‚Üí "VIP Customers" |
| **Persona** | M√¥ t·∫£ ng·∫Øn g·ªçn ƒë·∫∑c ƒëi·ªÉm kh√°ch h√†ng c·ª•m | "Kh√°ch h√†ng trung ni√™n, mua s·∫Øm th∆∞·ªùng xuy√™n ƒë·ªì gia d·ª•ng" |
| **Actionable Insights** | Th√¥ng tin c√≥ th·ªÉ h√†nh ƒë·ªông ƒë∆∞·ª£c | "N√™n g·ª≠i email ∆∞u ƒë√£i v√†o cu·ªëi tu·∫ßn" |
| **Marketing Strategy** | Chi·∫øn l∆∞·ª£c ti·∫øp th·ªã c·ª• th·ªÉ cho c·ª•m | Bundle promotion, Cross-sell, Retention campaign |
| **Customer Lifetime Value** | Gi√° tr·ªã kh√°ch h√†ng trong to√†n b·ªô v√≤ng ƒë·ªùi | CLV = Frequency √ó Monetary |

#### **üìç V·ªã tr√≠ trong h·ªá th·ªëng:**

T·∫°o file m·ªõi: `notebooks/cluster_interpretation.ipynb` ho·∫∑c th√™m v√†o cu·ªëi `clustering_from_rules.ipynb`

#### **üîß H∆∞·ªõng d·∫´n th·ª±c hi·ªán:**

**B∆∞·ªõc 3.1: Ph√¢n t√≠ch v√† ƒë·∫∑t t√™n**

```python
# Cell: ƒê·∫∑t t√™n v√† m√¥ t·∫£ c·ª•m
cluster_profiles = {
    0: {
        "name_en": "Casual Shoppers",
        "name_vi": "Kh√°ch H√†ng Th∆∞·ªùng",
        "size": 3797,
        "percentage": 96.84,
        "rfm_profile": {
            "recency": 45,
            "frequency": 3,
            "monetary": 300
        },
        "persona": "Kh√°ch h√†ng mua s·∫Øm kh√¥ng th∆∞·ªùng xuy√™n, gi√° tr·ªã ƒë∆°n h√†ng th·∫•p, ch·ªß y·∫øu mua ƒë·ªì trang tr√≠ nh·ªè l·∫ª",
        "top_products": ["CANDLE", "GIFT CARD", "PAPER NAPKINS"],
        "behavior": "Mua theo nhu c·∫ßu ƒë·ªôt xu·∫•t, kh√¥ng c√≥ pattern r√µ r√†ng",
        "marketing_strategy": {
            "objective": "TƒÉng t·∫ßn su·∫•t mua h√†ng v√† gi√° tr·ªã ƒë∆°n h√†ng",
            "tactics": [
                "Email marketing v·ªõi bundle deals (mua 2 t·∫∑ng 1)",
                "Cross-sell: g·ª£i √Ω s·∫£n ph·∫©m li√™n quan khi checkout",
                "Loyalty program: t√≠ch ƒëi·ªÉm ƒë·ªÉ khuy·∫øn kh√≠ch quay l·∫°i",
                "Seasonal campaigns: g·ª≠i catalog v√†o d·ªãp l·ªÖ"
            ],
            "expected_outcome": "TƒÉng Frequency t·ª´ 3 l√™n 5 ƒë∆°n/nƒÉm, tƒÉng Monetary 20%"
        },
        "budget_allocation": "40% (c·ª•m l·ªõn nh·∫•t)",
        "kpi": "Conversion rate, Average order value"
    },
    
    1: {
        "name_en": "VIP High-Value Customers",
        "name_vi": "Kh√°ch H√†ng VIP",
        "size": 124,
        "percentage": 3.16,
        "rfm_profile": {
            "recency": 15,
            "frequency": 20,
            "monetary": 5000
        },
        "persona": "Kh√°ch h√†ng trung th√†nh, mua s·∫Øm th∆∞·ªùng xuy√™n, gi√° tr·ªã cao, th√≠ch b·ªô s∆∞u t·∫≠p cao c·∫•p",
        "top_products": ["REGENCY TEACUP SET", "CERAMIC STORAGE JAR", "VINTAGE ALARM CLOCK"],
        "behavior": "Mua theo b·ªô s∆∞u t·∫≠p, quan t√¢m ch·∫•t l∆∞·ª£ng h∆°n gi√° c·∫£",
        "marketing_strategy": {
            "objective": "Gi·ªØ ch√¢n v√† tƒÉng gi√° tr·ªã lifetime",
            "tactics": [
                "VIP treatment: early access to new collections",
                "Personal shopper service: t∆∞ v·∫•n 1-1",
                "Exclusive events: private sale, product launch",
                "Premium loyalty tier: cashback 10%, free shipping",
                "Birthday/anniversary gifts"
            ],
            "expected_outcome": "Retention rate 95%+, tƒÉng Monetary 30%"
        },
        "budget_allocation": "60% (ROI cao nh·∫•t)",
        "kpi": "Customer retention rate, CLV"
    }
}

# In ra b·∫£ng t√≥m t·∫Øt
import json
print("="*80)
print("B·∫¢NG T√ìM T·∫ÆT PH√ÇN KH√öC KH√ÅCH H√ÄNG")
print("="*80)

for cluster_id, profile in cluster_profiles.items():
    print(f"\nüè∑Ô∏è  CLUSTER {cluster_id}: {profile['name_en']} ({profile['name_vi']})")
    print("-" * 80)
    print(f"üìä Quy m√¥: {profile['size']} kh√°ch ({profile['percentage']:.2f}%)")
    print(f"üìà RFM Profile: R={profile['rfm_profile']['recency']}, F={profile['rfm_profile']['frequency']}, M=${profile['rfm_profile']['monetary']}")
    print(f"üë§ Persona: {profile['persona']}")
    print(f"üõçÔ∏è  Top Products: {', '.join(profile['top_products'][:3])}")
    print(f"\nüéØ Chi·∫øn l∆∞·ª£c Marketing:")
    print(f"   M·ª•c ti√™u: {profile['marketing_strategy']['objective']}")
    print(f"   Chi·∫øn thu·∫≠t:")
    for i, tactic in enumerate(profile['marketing_strategy']['tactics'], 1):
        print(f"     {i}. {tactic}")
    print(f"   K·∫øt qu·∫£ k·ª≥ v·ªçng: {profile['marketing_strategy']['expected_outcome']}")
    print(f"üí∞ Ph√¢n b·ªï ng√¢n s√°ch: {profile['budget_allocation']}")
    print(f"üìä KPI: {profile['kpi']}")

# L∆∞u ra file JSON ƒë·ªÉ d√πng cho dashboard
with open('data/processed/cluster_profiles.json', 'w', encoding='utf-8') as f:
    json.dump(cluster_profiles, f, ensure_ascii=False, indent=2)
```

**B∆∞·ªõc 3.2: T·∫°o b·∫£ng t·ªïng h·ª£p cho b√°o c√°o**

```python
# Cell: B·∫£ng marketing strategy
strategy_table = pd.DataFrame([
    {
        'Cluster': f"{cid}: {p['name_vi']}",
        'Size': f"{p['size']} ({p['percentage']:.1f}%)",
        'RFM': f"R{p['rfm_profile']['recency']}/F{p['rfm_profile']['frequency']}/M${p['rfm_profile']['monetary']}",
        'Persona': p['persona'][:60] + "...",
        'Strategy': p['marketing_strategy']['tactics'][0][:50] + "...",
        'Budget': p['budget_allocation']
    }
    for cid, p in cluster_profiles.items()
])

print("\n" + "="*100)
print("B·∫¢NG CHI·∫æN L∆Ø·ª¢C MARKETING")
print("="*100)
print(strategy_table.to_markdown(index=False))

# Xu·∫•t ra Excel
strategy_table.to_excel('reports/marketing_strategy.xlsx', index=False)
```

---

### **B∆Ø·ªöC 4: So S√°nh C√°c Thu·∫≠t To√°n Ph√¢n C·ª•m (N√¢ng Cao)**

#### **üìö Gi·∫£i th√≠ch thu·∫≠t ng·ªØ:**

| Thu·∫≠t ng·ªØ | Gi·∫£i th√≠ch |
|-----------|------------|
| **K-Means** | Ph√¢n c·ª•m theo kho·∫£ng c√°ch Euclidean, gi·∫£ ƒë·ªãnh c·ª•m h√¨nh c·∫ßu |
| **Agglomerative (Hierarchical)** | Ph√¢n c·ª•m ph√¢n c·∫•p t·ª´ d∆∞·ªõi l√™n, t·∫°o dendrogram |
| **DBSCAN** | Density-based clustering, t√¨m c·ª•m theo m·∫≠t ƒë·ªô, t·ª± ƒë·ªông t√¨m outliers |
| **Silhouette Score** | ƒêo ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m, t·ª´ -1 ƒë·∫øn 1, c√†ng cao c√†ng t·ªët |
| **Davies-Bouldin Index** | T·ª∑ l·ªá within/between cluster variance, c√†ng th·∫•p c√†ng t·ªët |
| **Calinski-Harabasz** | T·ª∑ l·ªá between/within variance, c√†ng cao c√†ng t·ªët |

#### **üìç V·ªã tr√≠ trong h·ªá th·ªëng:**

T·∫°o file m·ªõi: `notebooks/clustering_comparison.ipynb`

#### **üîß H∆∞·ªõng d·∫´n th·ª±c hi·ªán:**

```python
# Cell: So s√°nh thu·∫≠t to√°n
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import time

# Load features
X = np.load('data/processed/X_features.npy')  # L∆∞u t·ª´ notebook clustering

results = []

# 1. K-Means v·ªõi K kh√°c nhau
for k in [3, 4, 5, 6]:
    start = time.time()
    km = KMeans(n_clusters=k, random_state=42, n_init='auto')
    labels = km.fit_predict(X)
    elapsed = time.time() - start
    
    results.append({
        'Algorithm': f'K-Means (K={k})',
        'N_Clusters': k,
        'Silhouette': silhouette_score(X, labels),
        'Davies-Bouldin': davies_bouldin_score(X, labels),
        'Calinski-Harabasz': calinski_harabasz_score(X, labels),
        'Time (s)': elapsed
    })

# 2. Agglomerative
for k in [3, 4, 5, 6]:
    start = time.time()
    agg = AgglomerativeClustering(n_clusters=k, linkage='ward')
    labels = agg.fit_predict(X)
    elapsed = time.time() - start
    
    results.append({
        'Algorithm': f'Agglomerative (K={k})',
        'N_Clusters': k,
        'Silhouette': silhouette_score(X, labels),
        'Davies-Bouldin': davies_bouldin_score(X, labels),
        'Calinski-Harabasz': calinski_harabasz_score(X, labels),
        'Time (s)': elapsed
    })

# 3. DBSCAN v·ªõi eps kh√°c nhau
for eps in [0.5, 1.0, 1.5]:
    start = time.time()
    db = DBSCAN(eps=eps, min_samples=10)
    labels = db.fit_predict(X)
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    elapsed = time.time() - start
    
    if n_clusters > 1:
        results.append({
            'Algorithm': f'DBSCAN (eps={eps})',
            'N_Clusters': n_clusters,
            'Silhouette': silhouette_score(X, labels),
            'Davies-Bouldin': davies_bouldin_score(X, labels),
            'Calinski-Harabasz': calinski_harabasz_score(X, labels),
            'Time (s)': elapsed
        })

# T·∫°o b·∫£ng so s√°nh
comparison_df = pd.DataFrame(results)
comparison_df = comparison_df.round(3)
comparison_df = comparison_df.sort_values('Silhouette', ascending=False)

print("="*100)
print("B·∫¢NG SO S√ÅNH C√ÅC THU·∫¨T TO√ÅN PH√ÇN C·ª§M")
print("="*100)
print(comparison_df.to_markdown(index=False))

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
comparison_df.plot(x='Algorithm', y='Silhouette', kind='bar', ax=axes[0], legend=False)
axes[0].set_title('Silhouette Score (higher is better)')
axes[0].set_ylabel('Score')

comparison_df.plot(x='Algorithm', y='Davies-Bouldin', kind='bar', ax=axes[1], legend=False, color='orange')
axes[1].set_title('Davies-Bouldin Index (lower is better)')

comparison_df.plot(x='Algorithm', y='Time (s)', kind='bar', ax=axes[2], legend=False, color='green')
axes[2].set_title('Execution Time')

plt.tight_layout()
plt.savefig('figures/algorithm_comparison.png', dpi=150)
plt.show()

# K·∫øt lu·∫≠n
best_row = comparison_df.iloc[0]
print(f"\nüèÜ Thu·∫≠t to√°n t·ªët nh·∫•t: {best_row['Algorithm']}")
print(f"   Silhouette: {best_row['Silhouette']:.3f}")
print(f"   Davies-Bouldin: {best_row['Davies-Bouldin']:.3f}")
print(f"   S·ªë c·ª•m: {best_row['N_Clusters']}")
```

---

### **B∆Ø·ªöC 5: X√¢y D·ª±ng Streamlit Dashboard (N√¢ng Cao)**

#### **üìö Gi·∫£i th√≠ch thu·∫≠t ng·ªØ:**

| Thu·∫≠t ng·ªØ | Gi·∫£i th√≠ch |
|-----------|------------|
| **Streamlit** | Framework Python ƒë·ªÉ t·∫°o web app data science nhanh ch√≥ng |
| **Dashboard** | B·∫£ng ƒëi·ªÅu khi·ªÉn hi·ªÉn th·ªã metrics v√† visualizations |
| **Interactive Filter** | B·ªô l·ªçc t∆∞∆°ng t√°c (dropdown, slider) |
| **Real-time Update** | C·∫≠p nh·∫≠t bi·ªÉu ƒë·ªì theo selection |

#### **üìç V·ªã tr√≠ trong h·ªá th·ªëng:**

T·∫°o file m·ªõi: `app/dashboard.py`

#### **üîß H∆∞·ªõng d·∫´n th·ª±c hi·ªán:**

**B∆∞·ªõc 5.1: T·∫°o file dashboard**

```python
# File: app/dashboard.py
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import json

# Config
st.set_page_config(page_title="Customer Segmentation Dashboard", layout="wide")

# Load data
@st.cache_data
def load_data():
    clusters = pd.read_csv("../data/processed/customer_clusters_from_rules.csv")
    rules = pd.read_csv("../data/processed/rules_fpgrowth_filtered.csv")
    with open("../data/processed/cluster_profiles.json", 'r', encoding='utf-8') as f:
        profiles = json.load(f)
    return clusters, rules, profiles

clusters_df, rules_df, cluster_profiles = load_data()

# Sidebar
st.sidebar.title("üéõÔ∏è B·ªô L·ªçc")
selected_cluster = st.sidebar.selectbox(
    "Ch·ªçn c·ª•m kh√°ch h√†ng:",
    options=["T·∫•t c·∫£"] + sorted(clusters_df['cluster'].unique().tolist())
)

# Main
st.title("üõçÔ∏è Customer Segmentation Dashboard")
st.markdown("Ph√¢n kh√∫c kh√°ch h√†ng d·ª±a tr√™n Association Rules")

# Overview
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("T·ªïng kh√°ch h√†ng", f"{len(clusters_df):,}")
with col2:
    st.metric("S·ªë c·ª•m", clusters_df['cluster'].nunique())
with col3:
    avg_monetary = clusters_df['Monetary'].mean()
    st.metric("Avg Monetary", f"${avg_monetary:,.2f}")
with col4:
    avg_freq = clusters_df['Frequency'].mean()
    st.metric("Avg Frequency", f"{avg_freq:.1f}")

# Cluster Distribution
st.subheader("üìä Ph√¢n B·ªë C·ª•m")
cluster_counts = clusters_df['cluster'].value_counts().sort_index()
fig_dist = px.bar(
    x=cluster_counts.index, 
    y=cluster_counts.values,
    labels={'x': 'Cluster', 'y': 'Number of Customers'},
    title="Cluster Size Distribution"
)
st.plotly_chart(fig_dist, use_container_width=True)

# Cluster Detail
if selected_cluster != "T·∫•t c·∫£":
    st.subheader(f"üîç Chi Ti·∫øt Cluster {selected_cluster}")
    
    # Get profile
    profile = cluster_profiles[str(selected_cluster)]
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"**T√™n:** {profile['name_vi']} ({profile['name_en']})")
        st.markdown(f"**Quy m√¥:** {profile['size']} kh√°ch ({profile['percentage']:.2f}%)")
        st.markdown(f"**Persona:** {profile['persona']}")
    
    with col2:
        st.markdown("**RFM Profile:**")
        st.markdown(f"- Recency: {profile['rfm_profile']['recency']} ng√†y")
        st.markdown(f"- Frequency: {profile['rfm_profile']['frequency']} ƒë∆°n")
        st.markdown(f"- Monetary: ${profile['rfm_profile']['monetary']:,}")
    
    # Marketing Strategy
    st.markdown("### üéØ Chi·∫øn L∆∞·ª£c Marketing")
    st.info(f"**M·ª•c ti√™u:** {profile['marketing_strategy']['objective']}")
    
    st.markdown("**Chi·∫øn thu·∫≠t:**")
    for i, tactic in enumerate(profile['marketing_strategy']['tactics'], 1):
        st.markdown(f"{i}. {tactic}")
    
    st.success(f"**K·∫øt qu·∫£ k·ª≥ v·ªçng:** {profile['marketing_strategy']['expected_outcome']}")
    
    # Top Rules
    st.markdown("### üìã Top 10 Lu·∫≠t K·∫øt H·ª£p")
    # Filter rules for this cluster (simplified - actual implementation needs rule activation matrix)
    st.dataframe(rules_df.head(10)[['antecedents_str', 'consequents_str', 'support', 'confidence', 'lift']])

else:
    # Compare all clusters
    st.subheader("üìä So S√°nh C√°c C·ª•m")
    
    comparison = clusters_df.groupby('cluster').agg({
        'Recency': 'mean',
        'Frequency': 'mean',
        'Monetary': 'mean'
    }).reset_index()
    
    fig = go.Figure()
    fig.add_trace(go.Bar(name='Recency', x=comparison['cluster'], y=comparison['Recency']))
    fig.add_trace(go.Bar(name='Frequency', x=comparison['cluster'], y=comparison['Frequency']))
    fig.add_trace(go.Bar(name='Monetary', x=comparison['Monetary']/100, y=comparison['cluster']))
    fig.update_layout(barmode='group', title="RFM Comparison Across Clusters")
    st.plotly_chart(fig, use_container_width=True)

# Footer
st.markdown("---")
st.markdown("üí° D·ª±a tr√™n FP-Growth Association Rules + K-Means Clustering")
```

**B∆∞·ªõc 5.2: Ch·∫°y dashboard**

```bash
cd app
streamlit run dashboard.py
```

Dashboard s·∫Ω m·ªü t·∫°i `http://localhost:8501`

---

## üìù CHECKLIST HO√ÄN CH·ªàNH

### **B·∫Øt bu·ªôc:**
- [ ] **‚úÖ Tr√¨nh b√†y r√µ c√°ch ch·ªçn lu·∫≠t**: Top-K, sort_by, ng∆∞·ª°ng l·ªçc, l√Ω do
- [ ] **‚úÖ B·∫£ng 10 lu·∫≠t ti√™u bi·ªÉu**: C√≥ ƒë·∫ßy ƒë·ªß support, confidence, lift
- [ ] **‚úÖ Ph√¢n t√≠ch ph√¢n b·ªë lu·∫≠t**: Histogram c·ªßa support/confidence/lift
- [ ] **‚úÖ T·∫°o ‚â•2 bi·∫øn th·ªÉ feature engineering**
- [ ] **‚úÖ So s√°nh c√°c bi·∫øn th·ªÉ**: B·∫£ng t·ªïng h·ª£p v·ªõi Silhouette score
- [ ] **‚úÖ Profiling t·ª´ng c·ª•m**: Th·ªëng k√™ RFM + top rules
- [ ] **‚úÖ ƒê·∫∑t t√™n c·ª•m**: Ti·∫øng Anh + ti·∫øng Vi·ªát
- [ ] **‚úÖ M√¥ t·∫£ persona**: 1-2 c√¢u cho m·ªói c·ª•m
- [ ] **‚úÖ Chi·∫øn l∆∞·ª£c marketing**: C·ª• th·ªÉ cho t·ª´ng c·ª•m
- [ ] **‚úÖ Tr·ª±c quan h√≥a 2D**: PCA/SVD scatter plot
- [ ] Silhouette score > 0.25
- [ ] M·ªói c·ª•m c√≥ √≠t nh·∫•t 5% t·ªïng kh√°ch h√†ng

### **N√¢ng cao (ƒëi·ªÉm t·ªëi ƒëa):**
- [ ] **‚úÖ So s√°nh thu·∫≠t to√°n**: K-Means vs Agglomerative vs DBSCAN
- [ ] **‚úÖ Dashboard Streamlit**: Interactive visualization
- [ ] Th·ª≠ basket/product/rule clustering

---

## üîó T√ÄI LI·ªÜU THAM KH·∫¢O

- [K-Means Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
- [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)
- [MLxtend Association Rules](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/)
- [RFM Analysis Guide](https://www.optimove.com/resources/learning-center/rfm-segmentation)

---

**C·∫≠p nh·∫≠t l·∫ßn cu·ªëi**: December 29, 2025  
**T√°c gi·∫£**: AI Assistant for DataMining Project
